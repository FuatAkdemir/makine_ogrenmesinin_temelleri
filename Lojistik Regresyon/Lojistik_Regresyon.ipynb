{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lojistik Regresyon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH_PR4pWGWdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Kullanılan kütüphaneler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k0GsKXmgPbin",
        "colab": {}
      },
      "source": [
        "# Google Drive  ortamına bağlanma\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X36wpV3qtmsB",
        "colab_type": "text"
      },
      "source": [
        "# Sınıflandırma\n",
        "Neslihan Oflaz ve Duygu Can\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        ">> Makine öğrenmesi ve istatistik alanlarında *sınıflandırma problemi*, temelde yeni bir gözlemin hangi kategoriye ait olduğu sorusuyla ilgilenir. Bir sınıflandırma problemini çözerken elimizdeki gözlemlerden ve bilinen kategorilerinden oluşan bir çalıştırma seti (training set) kullanırız.\n",
        "\n",
        "Biz bu çalışmada öncelikle ikili sınıflandırmaya yoğunlaşacağız (Birden fazla sınıftan/kategoriden oluşan  problemler  **çoklu sınıflandırma** diye adlandırılır).\n",
        "\n",
        "**Örnekler:**\n",
        "\n",
        "+ E-posta: İstenmeyen/İstenen?\n",
        "+ Online İşlemler: Sahte/Sahte Değil?\n",
        "+ Tümör: Kötü Huylu/İyi Huylu?\n",
        "\n",
        "Sınıflandırma problemi bir regresyon problemi olarak ele alabiliriz, yani elimizdeki değişkenlere bağlı olarak yeni bir değişken tahmin etmeye çalışabiliriz. İkili sınıflandırmalarda tahmin etmeye çalıştığımız değişkene *y* dersek:\n",
        "\n",
        "$$ y \\in \\{0, 1\\} $$\n",
        "$$ 0: \\mbox{Negatif Sınıf}$$\n",
        "$$ 1: \\mbox{Pozitif Sınıf}$$\n",
        "\n",
        "şeklinde iki değer kullanarak sınıflarımızı belirtebiliriz. Bir regresyon problemi olsa da  sınıflandırma problemlerinde kullanacağımız yöntem doğrusal regresyon   değildir çünkü  girdi ve tahmin arasında aradığımız ilişki doğrusal bir ilişki değildir. Ayrıca doğrusal regresyon sonucu sürekli olacağı için tahminlerimiz kesikli olay uzayımızın dışına düşebilir. Sınflandırma problemini çözmek için uygun olan regresyon algoritması burada bahsedeceğimiz **Lojistik Regresyon**'dur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7znTMELEt0W",
        "colab_type": "text"
      },
      "source": [
        "## İkili Sınıflandırma Örneği\n",
        "\n",
        "Bu çalışmada örnek olarak kullanacağımız  veride bir üniversitenin master programına başvuran öğrencilerin iki sınavdan (ALES, TOEFL ya da LES gibi) aldıkları puanlar ve programa kabul edilip edilmedikleri bilgisi bulunuyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_MirzNuQfDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Örnek 1 için eğitme verisinin okunması\n",
        "\n",
        "veri1 = pd.read_csv(\"/content/drive/My Drive/Makine_Öğrenmesi_Günü/data/log_reg_ex1.csv\", names=['sinav_1','sinav_2', 'kabul'])\n",
        "veri1.head(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwXB5_BTsKQD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Örnek-1 : Kabul/Red Edilen Öğrencilerin Sınav Notu Dağılımları\n",
        "# Verinin Görselleştirilmesi\n",
        "fig, ax = plt.subplots(figsize=(7,6))\n",
        "\n",
        "x_kabul = veri1[\"sinav_1\"].loc[veri1['kabul']==1]\n",
        "y_kabul = veri1[\"sinav_2\"].loc[veri1['kabul']==1] \n",
        "x_red = veri1[\"sinav_1\"].loc[veri1['kabul']==0] \n",
        "y_red = veri1[\"sinav_2\"].loc[veri1['kabul']==0] \n",
        "\n",
        "ax.scatter(x_kabul , y_kabul, marker= r'+', color='Green', label='Kabul')\n",
        "ax.scatter(x_red , y_red,  marker= r'o',color='Red', label='Red')\n",
        "ax.set_xlabel('Sınav 1')\n",
        "ax.set_ylabel('Sınav 2')\n",
        "ax.legend();\n",
        "#ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x0cNZd0_rEG",
        "colab_type": "text"
      },
      "source": [
        "Amacımız bu öğrenciler arasında Sınav 1 ve Sınav 2'den  aldıkları  notlara göre kabul alan ve reddedilen öğrenciler olmak üzere iki sınıfa ayırmak. Dahası herhangi bir öğrencinin aldığı sınav notlarına bakarak kabul edilme olasılığını hesaplayabilmek."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVhh9hy0C966",
        "colab_type": "text"
      },
      "source": [
        "Öncelikle kabul edilen ve reddedilen öğrenciler arasında doğrusal bir sınır çizmeye çalışalım, bu sınıra karar sınırı diyeceğiz. Şu anda incelediğimiz örnekte sadece iki nitelik olduğundan (bu örnekte sınavlar) grafikten yararlanarak elimizle de çizebileceğimiz bu sınırı daha sonra Lojistik Regresyon algoritması sayesinde herhangi bir nitelik sayısı için de belirleyebiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "_5ECoZZ5S4K4",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "# \n",
        "fig, ax = plt.subplots(figsize=(7,6))\n",
        "\n",
        "x_kabul = veri1[\"sinav_1\"].loc[veri1['kabul']==1]\n",
        "y_kabul = veri1[\"sinav_2\"].loc[veri1['kabul']==1] \n",
        "x_red = veri1[\"sinav_1\"].loc[veri1['kabul']==0] \n",
        "y_red = veri1[\"sinav_2\"].loc[veri1['kabul']==0] \n",
        "\n",
        "ax.scatter(x_kabul , y_kabul, marker= r'+', color='Green', label='Kabul')\n",
        "ax.scatter(x_red , y_red,  marker= r'o',color='Red', label='Red')\n",
        "\n",
        "x= np.arange(0,100,0.1)\n",
        "y= (24 - 0.2*x )/0.2 \n",
        "ax.plot(x,y)\n",
        "\n",
        "ax.set_xlabel('Sınav 1')\n",
        "ax.set_ylabel('Sınav 2')\n",
        "ax.legend()\n",
        "ax.set_xlim([25, 100])\n",
        "ax.set_ylim([25, 100])\n",
        "ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zc458JvuIaO",
        "colab_type": "text"
      },
      "source": [
        "# Karar Sınırı  \n",
        "\n",
        "> İki sınıflı bir istatistiksel sınıflandırma probleminde, bir karar sınırı veya karar yüzeyi, temel vektör uzayını her bir sınıf için bir tane olmak üzere iki kümeye ayıran bir (hiper) yüzeydir.\n",
        "\n",
        "Daha basit bir anlatımla *Karar Sınırı*, farklı sınıfları birbirinden ayıran sınırdır. Elimizde iki nitelik olduğunda karar sınırımızı bir doğru denklemi şeklinde ifade edebiliriz:\n",
        "\n",
        "$$ \\theta^T x =  \\theta_0+\\theta_1 x_1  + \\theta_2 x_2 = 0 $$\n",
        "\n",
        "Yani karar sınırının grafiği\n",
        "\n",
        "$$\n",
        "\\theta^T = \\left[\\begin{array}{ccc}\n",
        "\\theta_0 \\\\\n",
        "\\theta_1 \\\\\n",
        "\\theta_2 \n",
        "\\end{array} \\right]\n",
        "$$\n",
        "\n",
        "vektorünün bileşenleri tarafından belirlenir ve amacımız lojistik regresyon lgoritması kullanarak  bu parametreleri belirlemektir.\n",
        "\n",
        "Yukarıdaki örnekte  çizdiğimiz karar sınırı $  -24+0.2 x_1  + 0.2 x_2 = 0 $ şeklinde yazılabilir. Bu doğrunun üzerinde kalan alanda bulunan veri noktaları kabul alan öğrencileri belirtirken (y=1), bu doğrunun altında kalanlar ise kabul almayan öğrencileri (y=0) belirtir.\n",
        "\n",
        "Daha genel ifade edecek olursak\n",
        "\n",
        "$$ \\theta^T x \\geq 0 \\implies y=1 $$\n",
        "$$ \\theta^T x \\lt 0 \\implies y=0 $$\n",
        "\n",
        "olur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azZ5JcGwty6U",
        "colab_type": "text"
      },
      "source": [
        "# Hipotez Gösterimi\n",
        "\n",
        "Doğrusal regresyonda olduğu gibi lojistik regresyonda da bir hipotezden yararlanırız. Bu bölümde hipotezimizi göstermek için kullanacağımız fonksiyonu çalışacağız. Hipotezimizin bize öğrencilerin kabul edilme olasılığını vermesini, bu yüzden de $h_\\Theta (x)$ şeklinde gösterilen hipotezimizin 0 ve 1 değerleri arasında kalmasını isteriz:\n",
        "\n",
        "$$ 0 \\leq h_\\Theta(x) \\leq 1 $$\n",
        "\n",
        "Bu şartları sağlayan seçimlerden en çok kullanılanı $h_\\Theta (x) = g(\\Theta^{T}x)$'tir. g fonksiyonu sigmoid ya da lojistik fonksiyon olarak adlandırılır ve \n",
        "\n",
        "$$sigmoid(z)= \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "şeklinde verilir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf3aEqPr4cwP",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid Fonksiyonu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVqYJL_H4hfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid Fonksiyonu\n",
        "def sigmoid(z):\n",
        "  x = np.array(z)            # girdide tutarlılık için\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjzj9Bvwx3Q_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Sigmoid Fonksiyonunun Grafiği\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,6))\n",
        "\n",
        "x= np.arange(-8,8,0.1)\n",
        "y= sigmoid(x)\n",
        "\n",
        "ax.plot(x, y)\n",
        "plt.title('Sigmoid Fonksiyonu')\n",
        "ax.grid(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XEvYyScoJIR",
        "colab_type": "text"
      },
      "source": [
        "Daha önce de belirttiğimiz gibi $ h_\\theta(x) $, ise çıktımızın 1 olma ihtimalini verir.\n",
        "\n",
        "$$ h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$$\n",
        "\n",
        "Örneğin hipotez çıktısı $h_\\theta (x)=0.7 $ bize model çıktımızın (tahmin) %70 ihtimalle 1 olduğunu söyler. Bu bir ikili sınıflandırma problemi olduğu için y'nin 0 ya da 1 olması gerekir ve olasılıklar toplamı 1'e eşit olmalıdır. Aynı hipotez çıktısı tahminimizin %30 ihtimalle 0 olduğunu söyler.\n",
        "\n",
        "$$ h_\\theta(x)=P(y=1|x;\\theta)=1−P(y=0|x;\\theta) $$\n",
        "\n",
        "$$ P(y=0|x;θ)+P(y=1|x;\\theta)=1 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Zi6UiSqLSX",
        "colab_type": "text"
      },
      "source": [
        "Örnek okul problemimizde kesikli bir sınıflandırma yapabilmek için hipotez fonksiyonu çıktılarımızı aşağıdaki gibi çevirmemiz gerekir:\n",
        "\n",
        "$$ h_\\theta(x)\\geq 0.5 \\implies y=1 \\\\\n",
        "h_\\theta(x)\\lt 0.5 \\implies y=0 $$\n",
        "\n",
        "Dikkat ederseniz $h_\\theta(x) \\geq 0.5 $ iken y'yi 1 olarak tahmin ediyoruz. Bu da ancak sigmoid fonksiyonunun girdisinin pozitif olduğu durumlarda doğrudur. Bu da bizi başta bahsettiğimiz geometrik sınıflandırmaya götürür.\n",
        "\n",
        "$$ \\theta^T x \\geq 0 \\implies y=1 $$\n",
        "$$ \\theta^T x \\lt 0 \\implies y=0 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "jCcNd0sB9KFM",
        "colab": {}
      },
      "source": [
        "#@title Karar sınırına yakınlık ve olasılık ilişkisi\n",
        "# Verinin Görselleştirilmesi\n",
        "#fig, (ax1, ax2) = plt.subplots(figsize=(7,6))\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(17,7))\n",
        "#fig.suptitle('Horizontally stacked subplots')\n",
        "\n",
        "x_kabul = veri1[\"sinav_1\"].loc[veri1['kabul']==1]\n",
        "y_kabul = veri1[\"sinav_2\"].loc[veri1['kabul']==1] \n",
        "x_red = veri1[\"sinav_1\"].loc[veri1['kabul']==0] \n",
        "y_red = veri1[\"sinav_2\"].loc[veri1['kabul']==0] \n",
        "\n",
        "ax1.scatter(x_kabul , y_kabul, marker= r'+', color='Green', label='Kabul')\n",
        "ax1.scatter(x_red , y_red,  marker= r'o',color='Red', label='Red')\n",
        "\n",
        "ax1.set_title(\"Karar Sınırı\")\n",
        "ax1.set_xlabel('Sınav 1')\n",
        "ax1.set_ylabel('Sınav 2')\n",
        "ax1.set_xlim([25, 100])\n",
        "ax1.set_ylim([25, 100])\n",
        "ax1.grid(True)\n",
        "\n",
        "ax1.legend()\n",
        "#ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)   \n",
        "x= np.arange(0,100,0.1)\n",
        "y= (24 - 0.2*x )/0.2 \n",
        "ax1.plot(x,y, color=\"Black\")\n",
        "# specific points\n",
        "ax1.scatter(90,90,marker= r's', color='Cyan',s=50)\n",
        "ax1.scatter(60,70,marker= r'*', color='fuchsia', s=100)\n",
        "ax1.scatter(40,40,marker= r'v', color='Orange', s=70)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xs= np.arange(-15,15,0.1)\n",
        "ys= sigmoid(xs)\n",
        "theta_ornek = np.array([-24, 0.2, 0.2])\n",
        "kare = np.dot(theta_ornek,np.array([1, 90, 90]))\n",
        "yildiz = np.dot(theta_ornek,np.array([1, 60, 70]))\n",
        "ucgen = np.dot(theta_ornek,np.array([1, 40, 40]))\n",
        "\n",
        "ax2.plot(xs, ys)\n",
        "#specific points\n",
        "ax2.scatter(kare,sigmoid(kare),marker= r's', color='Cyan',s=50)\n",
        "ax2.scatter(ucgen,sigmoid(ucgen),marker= r'v', color='Orange', s=70)\n",
        "ax2.scatter(yildiz,sigmoid(yildiz),marker= r'*', color='fuchsia', s=100)\n",
        "#ax2.set_xlim([-12, 15])\n",
        "#ax2.set_ylim([-0.1, 1.1])\n",
        "ax2.set_title('Sigmoid')\n",
        "ax2.grid(True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPJrfKBpB6Zm",
        "colab_type": "text"
      },
      "source": [
        "Yukarıda sol taraftaki grafikte sarı üçgenle temsil edilen nokta sınav notları 40,40 olan öğrenciyi, eflatun yıldız sırasıyla 60, 70 olan öğrenciyi,\n",
        "turkuaz kare ise notları 90, 90 olan öğrenciyi temsil eder. Bu sol taraftaki grafikte işaretlendiği gibi bu öğrencilerin kabul olasılıkları\n",
        "sırasıyla 0, 0.9 ve 1 olarak hesaplanır. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LTO14EDmq2c",
        "colab_type": "text"
      },
      "source": [
        "## Doğrusal Olmayan Karar Sınırı\n",
        "\n",
        "Her sınıflandırma probleminde sınıflar doğrusal bir karar sınırı ile ayrılmak zorunda değildir. Örneğin mikroçip üreten bir araştırma şirketinin kalite kontrol testlerini ele alalım. Elimizde iki teste sokulan bu çiplerin kalite kontrolu geçip geçmediklerine dair bilgiler bulunmaktadır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usArod3pvNHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Teste tabi tutulan chiplerin test sonuçları\n",
        "\n",
        "veri2 = pd.read_csv(\"/content/drive/My Drive/Makine_Öğrenmesi_Günü/data/log_reg_ex2.csv\", names=['test_1','test_2', 'onay'])\n",
        "veri2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbS8gaBryyTi",
        "colab_type": "text"
      },
      "source": [
        "Bu verileri çizdirdiğimizde testten kalan ve geçen çiplerin doğrusal bir sınırla rahatlıkla ayrılamacağı görülmektedir. Peki, böyle bir veri kümesi verildiğinde lojistik regresyonu veriye nasıl uydurmalıyız?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtqcCuT2qjck",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Örnek-2: Kaliteli ve Kalitesiz Mikroçiplerin  Dağılımı\n",
        "# Decision Boundary\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "x_iyi= veri2[\"test_1\"].loc[veri2['onay']==1]\n",
        "y_iyi = veri2[\"test_2\"].loc[veri2['onay']==1] \n",
        "x_kotu = veri2[\"test_1\"].loc[veri2['onay']==0] \n",
        "y_kotu = veri2[\"test_2\"].loc[veri2['onay']==0] \n",
        "theta = np.linspace(0, 2 * np.pi, 50)\n",
        "\n",
        "ax.scatter(x_iyi , y_iyi, marker= r'+', color='Green', label='Kaliteli')\n",
        "ax.scatter(x_kotu , y_kotu,  marker= r'o',color='Red', label='Kalitesiz')\n",
        "\n",
        "ax.set_xlabel('Mikroçip Testi 1')\n",
        "ax.set_ylabel('Mikroçip Testi 2')\n",
        "ax.legend();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHbaFXZDuVFi",
        "colab_type": "text"
      },
      "source": [
        "Polinom regresyonunda, doğrusal olmayan uyumlama yapabilmek fazladan yüksek dereceli nitelikler eklemiştik. Benzer şekilde, lojistik regresyonla da eğri karar sınırları yakalayabilmek için böyle terimler ekleyeceğiz. Sonuçta sigmoid fonksiyonunun girdisinin doğrusal olmadığı durumlarda çıktısı da doğrusal değildir. Örneğin:\n",
        "\n",
        "$$ \\theta^T x =  \\theta_0+\\theta_1 x_1^2 + \\theta_2 x_2^2 + \\theta_3 x_1x_2$$\n",
        "\n",
        "Yukarıdaki denklemi 0'a eşitlediğimizde doğrusal olmayan bir *Karar Sınırı* elde ederiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_IRIH56UkRU",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Doğrusal Olmayan Karar Sınırı Örneği\n",
        "# Decision Boundary\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "x_iyi= veri2[\"test_1\"].loc[veri2['onay']==1]\n",
        "y_iyi = veri2[\"test_2\"].loc[veri2['onay']==1] \n",
        "x_kotu = veri2[\"test_1\"].loc[veri2['onay']==0] \n",
        "y_kotu = veri2[\"test_2\"].loc[veri2['onay']==0] \n",
        "theta = np.linspace(0, 2 * np.pi, 50)\n",
        "r= 1/2\n",
        "x = (r+0.27)* np.sin(theta-np.pi/6) + 0.10\n",
        "y = (r+0.12) * np.cos(theta) + 0.15\n",
        "\n",
        "plt.plot(x,y)\n",
        "ax.scatter(x_iyi , y_iyi, marker= r'+', color='Green', label='Kaliteli')\n",
        "ax.scatter(x_kotu , y_kotu,  marker= r'o',color='Red', label='Kalitesiz')\n",
        "\n",
        "ax.set_xlabel('Kalite Testi 1')\n",
        "ax.set_ylabel('Kalite Testi 2')\n",
        "ax.legend()\n",
        "a=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxk97gKPfdRF",
        "colab_type": "text"
      },
      "source": [
        "# Bedel Fonksiyonu\n",
        "\n",
        "Bu bölümde lojistik regresyonun parametrelerini veri setine uydurmak için gereken optimizasyon hedefini tanımlayacağız. Şimdiye kadar  *en iyi* karar sınırını ve *doğru* hipotezi veren parametreleri ($\\mathbf\\theta$ vektörünü oluşturan parametreleri), yani hep *en iyi* karar sınırını ve *doğru* hipotezimizi bildiğimizi varsayarak birtakım çıkarımlarda bulunduk. Gerçekten de örnekte kullandığımız parametreler bize iyi bir sınıflandırma sağlar. Fakat özellikle ikiden fazla niteliğin varlığında iyi bir karar sınırı verecek  parametreleri elle hesaplayarak belirlemek pek mümkün değildir.\n",
        "\n",
        "\n",
        "**Peki bu parametreleri ($\\theta$'ları) nasıl belirleriz?**\n",
        "+ *En hatasız* hipotezi verecek parametreleri seçerek. Bunu yaparken Doğrusal Regresyonda olduğu gibi bir bedel fonksiyonu tanımlayacağız. \n",
        "\n",
        "\n",
        "m adet örnek ve n adet nitelik içeren bir eğitim verisi(traning data) alalım:\n",
        "\n",
        "$$ \\mbox{Eğitim Verisi:} {(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}) } $$\n",
        "\n",
        "$$ \\begin{equation*}\n",
        "x \\in\n",
        "\\begin{bmatrix}\n",
        "x_{0} \\\\\n",
        "x_{1} \\\\\n",
        "\\vdots \\\\\n",
        "x_{n} \n",
        "\\end{bmatrix}\n",
        "\\end{equation*}$$\n",
        "\n",
        "$$ h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_b8Mly6McJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hipotez\n",
        "def hipotez(X, theta):\n",
        " return sigmoid(np.dot(X,theta)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtKQ-J0mpIlV",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Örnek-1 \n",
        "veri1.head(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVXce-tJizgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veri1.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doEEVkMdqWjP",
        "colab_type": "text"
      },
      "source": [
        "Örnek-1 için 2 nitelik vardır (sınav1 ve sınav2),  yani n=2'dir. Eğitim datasının sadece ilk 6 satırını göstersek de 100 örnek satırı (100 öğrenci) vardır, bu yüzden m=100'dür."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EieHM4NCh4qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Örnek-1 \n",
        "\n",
        "X0 = np.array(veri1[['sinav_1', 'sinav_2']])   # nitelikler\n",
        "y1 = np.array(veri1['kabul'])                  # gerçek değer vektörü\n",
        "m = len(y1)                                    \n",
        "birler = np.ones(m)                            \n",
        "X1 = np.column_stack((birler, X0))             # birler sutununun eklenmesi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2s-6ZXlpAZQ",
        "colab_type": "text"
      },
      "source": [
        "m adet örneğe n adet niteliğe sahip eğitim verisinde i. satır için $x_0^{(i)} =1$ olmak üzere:\n",
        "\n",
        "$$ \\begin{equation*}\n",
        "x^{(i)} =\n",
        "\\begin{bmatrix}\n",
        "x^{(i)}_{0} \\\\\n",
        "x^{(i)}_{1} \\\\\n",
        "\\vdots \\\\\n",
        "x^{(i)}_{n} \n",
        "\\end{bmatrix},  \\quad   y^{(i)}= 0 \\mbox{ veya } 1 \\mbox{'dir. }\n",
        "\\end{equation*}$$\n",
        "\n",
        "$$\\theta^T\\cdot x^{(i)} = \\sum_{j=0}^n \\theta_j x^{(i)}_j$$\n",
        "\n",
        "olmak üzere hipotezimiz \n",
        "$$ h_\\theta(x^{(i)}) = \\frac{1}{1+e^{-\\theta^T x^{(i)}}}$$ \n",
        "her satır için olasılık/tahmin verir.\n",
        "\n",
        "Ve her bir tahminin  gerçekten ne kadar saptığını $bedel(h_\\theta(x^{(i)}), y^{(i)})$ ile ölçersek toplam bedel\n",
        "\n",
        "$$ J(\\theta) = \\sum_{i=1}^m bedel(h_\\theta(x^{(i)}), y^{(i)}) $$\n",
        "\n",
        "olarak hesaplanır.\n",
        "\n",
        "Bedel fonksiyonunu Doğrusal Regresyon'un bedel fonksiyonunda olduğu gibi tahminin gerçek sınıftan sapmasının karesi olarak tanımlarsak konveks olmayan bir bedel fonksiyonu elde ederiz ve Dereceli Alçalma algoritmasının bu bedel fonksiyonunun global minimumunu bulması garanti değildir (lokal minimalara takılabilir). Bu yüzden *Lojistik Regresyon* için bedel fonksiyonu aşağıdaki gibi tanımlanır:\n",
        "\n",
        "\n",
        "$$\\begin{equation*}\\\\\n",
        "bedel(h_\\theta(x), y) = \\left\\{\n",
        "\\begin{array}{rl}\n",
        "-log(h_\\theta(x)) & \\text{eğer } y = 1,\\\\\n",
        "-log(1 -h_\\theta(x)) & \\text{eğer } y = 0.\n",
        "\\end{array} \\right.\n",
        "\\end{equation*}\n",
        "$$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udokSYpNBE12",
        "colab_type": "text"
      },
      "source": [
        "Gerçekte $y = 1$ iken, model tahmini $h_\\theta$ bire yaklaştıkça *Bedel* azalır; aksi yönde artar!\n",
        "\n",
        "Benzer şekilde $y = 0$ iken, model tahmini $h_\\theta$ sıfıra yaklaştıkça *Bedel* azalır; aksi yönde artar!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RK7Pto5JQVE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title J'nin hipoteze göre değişimi\n",
        "# Bedel fonksiyonunun y-1 ve y=0 durumundaki grafikleri\n",
        "fig, (ax1, ax2)= plt.subplots(1,2,figsize=(14,6))\n",
        "def J1(x):\n",
        "  #eps = 10^-3\n",
        "  return -np.log(x)\n",
        "def J0(x):\n",
        "  #eps = 10^-3\n",
        "  return -np.log(1-x)\n",
        "x = np.arange(0.0001,1,0.01)\n",
        "y = J1(x)\n",
        "ax1.plot(x,J1(x), \"-\")\n",
        "ax1.set_title(\"y=1 durumunda:\")\n",
        "ax2.plot(x,J0(x), \"-\")\n",
        "ax2.set_title(\"y=0 durumunda:\")\n",
        "a=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn61MHYWNFAJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi8QUnF3wOL3",
        "colab_type": "text"
      },
      "source": [
        "## Basitleştirilmiş Bedel Fonksiyonu\n",
        "\n",
        "**y = 1** ve **y = 0** durumları için ayrı ayrı yazılan *Bedel Fonksiyon*'unu tek bir formülde gösterebiliriz. \n",
        "\n",
        "$$ \\mathrm{bedel}(h_\\theta(x),y) = - y \\; \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x)) $$\n",
        "\n",
        "**y = 1** iken ikinci terim, **y=0** iken ilk terim yok olacaktır ve *Bedel Fonksiyonu* eski halini alacaktır. Sadece bu daha *sıkıştırılmış* bir gösterimdir.\n",
        "\n",
        "Konveks şekilli bu bedel fonksiyonu ayrıca istatistikten bildiğimiz En Yüksek Olabilirlik Kestirimi (Maximum Likelihood Estimation) ile elde edilebilir.\n",
        "\n",
        "Tüm *Bedel Fonksiyonu* ise aşağıdaki gibi ifade edilir:\n",
        "\n",
        "$$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m [y^{(i)} \\; \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69XmVeUgurHO",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "# Bedel fonksiyonu\n",
        "\n",
        "def bedel(theta, X, y):     # X1: bir sutunu eklenmiş nitelik matrisi; y: gerçek değer vektörü; theta: parametre vektörü;\n",
        "  m = len(y)      \n",
        "  h = hipotez(X, theta)  \n",
        "  eps = 0.0001              #log(0) hatasini onlemek icin\n",
        "\n",
        "  J = 1/m*(np.dot(y, -np.log(h+eps)) + np.dot(1-y,-np.log(1-h+eps)))\n",
        "\n",
        "  return J"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjixm_sR9Ikk",
        "colab_type": "text"
      },
      "source": [
        "Bu bedel fonksiyonu verilmişken, iki sınıfı birbirinden en iyi ayıran karar sınırını bulmak için fonksiyonu $\\theta$'ya göre minimize etmek gerekir. Böylelikle en iyi parametre seti bize en düşük bedeli verecektir. \n",
        "\n",
        "$$ \\underset{\\theta}{\\text{min}} J(\\theta)  $$\n",
        "\n",
        "En düşük bedeli veren parametre seti hipotez fonksiyonuna konulunca, yeni bir örnek, *x* için model tahminimizi bulabiliriz.\n",
        "$$ h_\\theta(x) = \\frac{1}{1+e^{-\\theta^T x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlsIhVOwdMgL",
        "colab_type": "text"
      },
      "source": [
        "Mesela yüksek lisansa kabul alma örneğinde,\n",
        "$$ \\theta = [-24\\ \\  0.2\\ \\ 0.2\\ ]$$ \n",
        "için bedeli hesaplayalım."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "yIc0smFmkCqQ",
        "colab": {}
      },
      "source": [
        "# TEST bedel fonksiyonu\n",
        "\n",
        "theta_ornek = np.array([-24, 0.2, 0.2])\n",
        "bedel_ornek = bedel(theta_ornek, X1, y1)\n",
        "print('bedel=', bedel_ornek.round(3))\n",
        "#passed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7cfjHYmJ_I0",
        "colab_type": "text"
      },
      "source": [
        "Düşük bir bedel değerine sahip hipotezimiz (modelimiz), iki sınıfı birbirinden iyi ayırmaktadır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xap09LL9gL2w",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title bedel = 0.218\n",
        "# \n",
        "fig, ax = plt.subplots(figsize=(5,4))\n",
        "\n",
        "x_kabul = veri1[\"sinav_1\"].loc[veri1['kabul']==1]\n",
        "y_kabul = veri1[\"sinav_2\"].loc[veri1['kabul']==1] \n",
        "x_red = veri1[\"sinav_1\"].loc[veri1['kabul']==0] \n",
        "y_red = veri1[\"sinav_2\"].loc[veri1['kabul']==0] \n",
        "x= np.arange(0,100,0.1)\n",
        "y= (24 - 0.2*x )/0.2 \n",
        "ax.scatter(x_kabul , y_kabul, marker= r'+', color='Green', label='Kabul')\n",
        "ax.scatter(x_red , y_red,  marker= r'o',color='Red', label='Red')\n",
        "\n",
        "ax.plot(x,y)\n",
        "ax.set_xlabel('Sınav 1')\n",
        "ax.set_ylabel('Sınav 2')\n",
        "ax.legend()\n",
        "ax.set_xlim([30, 100])\n",
        "ax.set_ylim([30, 100])\n",
        "ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGuCmeh4PIIk",
        "colab_type": "text"
      },
      "source": [
        "# Dereceli Alçalma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35FiTlh5psZs",
        "colab_type": "text"
      },
      "source": [
        "Bedeli minimize etmek için *Dereceli Alçalma* algoritmasını kullanacağız. Dereceli Alçalmanın genel algoritmasının aşağıdaki gibi olduğunu hatırlayalım.\n",
        "\\begin{align*}& Tekrar \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) \\newline & \\rbrace\n",
        "\\end{align*}\n",
        "Bu algoritmanın tüm parametreleri ($\\theta$) her adımda eş zamanlı güncellediğini unutmamak gerekir.\n",
        "Lojistik Regresyon bedel fonksiyonunu yerine koyup tekrar yazarsak:\n",
        "\n",
        "\\begin{align*} & Tekrar \\; \\lbrace \\newline & \\; \\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\newline & \\rbrace \\end{align*}\n",
        "\n",
        "eşitliğini elde ederiz. Bu denklem *Doğrusal Regresyon* parametre güncelleme kuralı ile aynı görünüşte olsa da $h_\\theta(x^{(i)})$ sigmoid formunda olacağı için aslında farklıdır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Oce7mgcmZF",
        "colab_type": "text"
      },
      "source": [
        "*Dereceli Alçalma* algoritmasının vektörize implementasyonu ($\\theta:= \\theta− \\frac{\\alpha}{m}X^T (g(Xθ)− \\vec{y})$) aşağıdaki  gibidir. Fakat pratikte çok yavaş çalıştığı için bedeli minimize etmek için bir optimizasyon paketinden yararlanacağız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoXfwUlGtX7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Derece fonksiyonu \n",
        "\n",
        "def derece(theta, X, y):\n",
        "  m = len(y)\n",
        "  h = hipotez(X,theta)\n",
        "  \n",
        "  # bedel derecesi\n",
        "  grad = 1/m*np.dot(np.transpose(X), (h-y))\n",
        "  \n",
        "  return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwFM0KaXu1NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "derece(theta_ornek, X1, y1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0evd2Mzef1iR",
        "colab": {}
      },
      "source": [
        "# Dereceli alçalma\n",
        "\n",
        "def dereceli_alcalt(it, alpha,  X, y):  # alpha: öğrenme parametresi\n",
        "  ## egitilmemis theta\n",
        "  m , n = X.shape\n",
        "  theta0 = np.zeros(n)\n",
        "\n",
        "  for i in range(0,it):                 # it: iterasyon sayısı\n",
        "    grad = derece(theta0, X, y)\n",
        "    theta0 = theta0 - alpha*grad\n",
        "\n",
        "  return theta0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdc3hCyCWebW",
        "colab_type": "text"
      },
      "source": [
        " Yavaş çalışmaması için dereceli alçalma fonksiyonunu az iterasyonla kullanırsak bedel fonksiyonu için pek de iyi bir sonuç alamadığımızı görüyoruz. Bu fonksiyonun doğru global minimuma yakınsamadan iterasyonun bitmesinden kaynaklanmaktadır."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEaqvbNADYrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10000 iterasyon\n",
        "theta_deneme = dereceli_alcalt(10000, 0.3, X1, y1)\n",
        "bedel(theta_deneme, X1, y1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jlg6r8l1K9e",
        "colab_type": "text"
      },
      "source": [
        "İterasyon sayısını artırdıkça cevap iyileşecektir. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKWsIOOj1J9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 100000 iterasyon\n",
        "theta_deneme = dereceli_alcalt(100000, 0.3, X1, y1)\n",
        "bedel(theta_deneme, X1, y1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T-t42a7f4PF",
        "colab_type": "text"
      },
      "source": [
        "Nitelikler aynı aralığa ölçeklendirilirse *Dereceli Alçalma* algoritması sonuca daha hızlı yakınsayacaktır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICNAsF_FeWtb",
        "colab_type": "text"
      },
      "source": [
        "### İLERİ OPTİMİZASYON TEKNİKLERİ\n",
        "\n",
        "*Eşlenik Gradyan*, *BFGS*, *L-BFGS* gibi karmaşık algoritmalar her iterasyonda uygun öğrenme hızını seçerek sonuca *Dereceli Azalım* algoritmasından daha hızlı yakınsarlar.\n",
        "\n",
        "Örneğin *SciPy* kütüphanesindeki *fmin_tnc* fonksiyonu *Newton Eşlenik Gradyan* algoritmasının implementasyonudur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSciQffUbm2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bedel fonksiyonunun fmin_tnc ile minimize edilmesi\n",
        "\n",
        "import scipy.optimize as op\n",
        "theta0 = np.zeros(3)  \n",
        "\n",
        "optim=  op.fmin_tnc(func=bedel, x0=theta0, args=(X1, y1),approx_grad=True)\n",
        "\n",
        "theta_op = optim[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKI4s7ANZYjq",
        "colab_type": "text"
      },
      "source": [
        "Hazır bir optimizayon kütüphanesinden kullandığımız fonksiyonla çok daha iyi bir sonuç elde ediyoruz:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWPybzRZCi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bedel(theta_op, X1, y1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boawh44Hd_jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fmin_tnc ile buldugumuz theta vektoru\n",
        "theta_op.round(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaEihYmud4NQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title bedel = 0.203\n",
        "# \n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "x_kabul = veri1[\"sinav_1\"].loc[veri1['kabul']==1]\n",
        "y_kabul = veri1[\"sinav_2\"].loc[veri1['kabul']==1] \n",
        "x_red = veri1[\"sinav_1\"].loc[veri1['kabul']==0] \n",
        "y_red = veri1[\"sinav_2\"].loc[veri1['kabul']==0] \n",
        "x= np.arange(0,100,0.1)\n",
        "y= (-theta_op[0] -theta_op[1] *x )/theta_op[2]\n",
        "ax.scatter(x_kabul , y_kabul, marker= r'+', color='Green', label='Kabul')\n",
        "ax.scatter(x_red , y_red,  marker= r'o',color='Red', label='Red')\n",
        "\n",
        "ax.plot(x,y)\n",
        "ax.set_xlabel('Sınav 1')\n",
        "ax.set_ylabel('Sınav 2')\n",
        "ax.legend()\n",
        "ax.set_xlim([30, 100])\n",
        "ax.set_ylim([30, 100])\n",
        "ax.tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCcLYRbbhdxx",
        "colab_type": "text"
      },
      "source": [
        "## Çok Sınıflı Sınıflandırma\n",
        "\n",
        "Şimdi ikiden fazla kategoriye sahip olduğumuzda verinin sınıflandırmasına değineceğiz. Mesela gelen e-postaları iş, aile, arkadaşlar ya da hobi olarak otomatik tasnif eden bir makine öğrenmesi modeli bir çoklu sınıflandırma problemidir.\n",
        "$Y = {0,1}$ yerine tanımımızı genişleteceğiz, $y = {0,1 ... n}$. Y = {0,1 ... n} olduğundan, sorunumuzu n + 1'e (+1 olarak bölüyoruz, çünkü dizin 0'da başlıyor) ikili sınıflandırma problemleri; her birinde, 'y'nin sınıflarımızdan birinin üyesi olma ihtimalini tahmin ediyoruz.\n",
        "\n",
        "\\begin{align*}& y \\in \\lbrace0, 1 ... n\\rbrace \\newline& h_\\theta^{(0)}(x) = P(y = 0 | x ; \\theta) \\newline& h_\\theta^{(1)}(x) = P(y = 1 | x ; \\theta) \\newline& \\cdots \\newline& h_\\theta^{(n)}(x) = P(y = n | x ; \\theta) \\newline& \\mathrm{prediction} = \\max_i( h_\\theta ^{(i)}(x) )\\newline\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja7A0Mg3hxLh",
        "colab_type": "text"
      },
      "source": [
        "## Birine Karşıt Hepsi (One-vs-All)\n",
        "\n",
        "Temel olarak bir sınıfı seçiyoruz ve ardından diğerlerini tek bir ikinci sınıfa ayırıyoruz. Bunu her seferinde ikili lojistik regresyon uygulayarak tekrar tekrar yapıyoruz ve sonra en yüksek değeri döndüren hipotezi tahminimiz olarak kullanıyoruz.\n",
        "\n",
        "Aşağıdaki resim, birinin 3 sınıfı nasıl sınıflandırabileceğini göstermektedir:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1vW3mXSJWGZnOuwtEi5k9IH4BwpcVse4r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sfiePS7aokhx"
      },
      "source": [
        "## Aşırı Öğrenme (Overfitting)\n",
        "\n",
        "Y'yi x ∈ R'den tahmin etme problemini düşünün. Aşağıdaki en soldaki şekil, bir $y = \\theta_0 + \\theta_1 x$ veri setine uydurma sonucunu göstermektedir. Verilerin gerçekten düz bir çizgide bulunmadığını görüyoruz ve bu nedenle uygunluk çok iyi değil.\n",
        " \n",
        "![](https://drive.google.com/uc?id=1vykgbJmIkxRUa5qfPmkhdpN6-jfO3_WR)\n",
        "\n",
        "Ortadaki şekil ikinci dereceden $ y = \\theta_0 + \\theta_1x + \\theta_2x^2$ denkleminin veriye uydurulmasıdır. Ne kadar çok yeni nitelik eklersek eğri noktalara daha iyi uyumsar.  En sağdaki şekil 5. dereceden bir polinomun uyumsamasıdır: $ y = \\sum_{j=0} ^5 \\theta_j x^j$. Artık eğri her noktadan geçse de bu model iyi bir tahminleyici değildir çünkü veriyi ezberlemiştir. Yeni bir örnek geldiğinde sonucu genelleyemez.\n",
        "Özetle en soldaki grafik yetersiz-uyum (underfitting), en sağdaki grafik ise aşırı-uyum (overfitting) örneğidir. \n",
        "\n",
        "Yetersiz uyum (underfitting) ya da yüksek önyargı (high bias), hipotez fonksiyonumuzun biçiminin veri eğilimi yakalayamasıdır. Genellikle çok basit veya çok az özellik kullanan bir işlevden kaynaklanır. Diğer uçta ise, aşırı-uydurma veya yüksek varyans, mevcut verilere uyan ancak yeni verileri öngörmek için iyi genelleme yapmayan bir hipotez fonksiyonundan kaynaklanır. Genellikle verilerle ilgisi olmayan birçok gereksiz eğri ve açı oluşturan karmaşık bir fonksiyonu model olarak kullanmaktan kaynaklanır.\n",
        "\n",
        "Bu terminoloji hem doğrusal hem de lojistik regresyona uygulanır. Aşırı uyum (overfitting) sorununu ile başa çıkmak için iki yol vardır:\n",
        "\n",
        "1. Nitelik sayısını azaltmak:\n",
        "  + Elle seçerek\n",
        "  + Bir model seçim algoritmasını kullanarak\n",
        "\n",
        "2. Düzenlileştirme\n",
        "  + Tüm nitelikleri koruyarak, ancak parametre $\\theta_j$ büyüklüklerini küçülterek\n",
        "  + Düzenlileştirme, pek çok biraz kullanışlı nitelikliğe sahip olduğumuzda iyi çalışır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26_Rmg5XzbIV",
        "colab_type": "text"
      },
      "source": [
        "## Düzenlileştirme \n",
        "\n",
        "Düzenlileştirme için *Bedel Fonksiyonu*'nun sonuna parametre büyüklüklerini sınırlayıcı bir terim ekleriz.\n",
        "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/Od9mobDaEeaCrQqTpeD5ng_4f5e9c71d1aa285c1152ed4262f019c1_Screenshot-2016-11-22-09.31.21.png?expiry=1572652800000&hmac=MM27fp24SjI7PLK9dY6cRJleWDnHFIbAm-Pxaz1Q_Hs)\n",
        "\n",
        "*Bedel Fonksiyonu*nun düzenlileştirme altındaki ifadesi şuna dönüşür:\n",
        "\n",
        "$$ J(\\theta) = \\frac{-1}{m}\\sum_{i=1}^m [y^{(i)} \\; \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] + \n",
        "\\frac{\\lambda}{2m}\\sum_{j=1}^m \\theta_j^2 $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4UjnNmkyfSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regülarize edilmiş bedel fonksiyonu\n",
        "\n",
        "def reg_bedel(theta, lmbda, X, y):             # X: nitelik matrisi, y: gerçek değer vektörü, theta: parametre vektörü, lmbda: regülarizasyon parametresi\n",
        "  m = len(y)\n",
        "  h = sigmoid(np.dot(X,theta))                 # hipotez\n",
        "  \n",
        "  eps = 0.0001                                 # log(0) hatasini onlemek icin\n",
        "  J0 = bedel(theta, X, y )\n",
        "  J_lmbda = lmbda/(2*m)*np.dot(theta, theta)   # regularizasyon  terimi\n",
        "  \n",
        "  return J0 + J_lmbda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTYL_URwtUpz",
        "colab_type": "text"
      },
      "source": [
        "*Bedel Fonksiyonu*'nu $\\theta$'ya göre minimize ettiğimizde ($ \\underset{\\theta}{\\text{min }} J(\\theta)$), sondaki toplam terimi $\\theta$'ları $\\lambda$, düzenlileştirme parametresi ile sınırlayacaktır. *Bedel Fonksiyonu*'muzu böyle değiştirdiğimizde *Dereceli Alçalma* algoritmamız da aşağıdaki gibi değişir:\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align*}  Tekrar  \\; \\lbrace \\newline  \\;\n",
        "&\\theta_0 := \\theta_0 - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\newline \n",
        "&\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \\newline & \\rbrace \\end{align*}\n",
        "\n",
        "Dikkat ederseniz sondaki toplam teriminin indeksi önyargı (bias) terimini atlamak için 1'den başlatılmıştır. Bu yüzden *Dereceli Alçalma* algoritması içinde $\\theta_0$ ayrıca güncelleştirilir. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfF7LWzBL_A3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regülarize edilmiş derece fonksiyonu\n",
        "\n",
        "def reg_derece(theta, X, y):\n",
        "  m = len(y)\n",
        "  h = hipotez(X,theta)                          # hipotez\n",
        "  \n",
        "  grad = 1/m*np.dot(np.transpose(X), (h-y))     # derece\n",
        "  grad_lmbda = lmbda/m*theta                    # regularizasyon terimi\n",
        "  \n",
        "  return grad+grad_lmbda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ejPzhu8Yokh0"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1fIQaDYsd0th2dlAPcl_PLI-iK_XsP41U)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPeug36WVtoy",
        "colab_type": "text"
      },
      "source": [
        "## Kaynakça\n",
        "\n",
        "+ [İstatistiki Sınıflandırma](https://www.wikizeroo.org/index.php?q=aHR0cHM6Ly90ci53aWtpcGVkaWEub3JnL3dpa2kvxLBzdGF0aXN0aWtpX3PEsW7EsWZsYW5kxLFybWE)\n",
        "+ [Decision Boundary](https://www.wikizeroo.org/index.php?q=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvRGVjaXNpb25fYm91bmRhcnk)\n",
        "\n",
        "+ [Deriving cost function using MLE :Why use log function?](https://math.stackexchange.com/questions/886555/deriving-cost-function-using-mle-why-use-log-function)"
      ]
    }
  ]
}